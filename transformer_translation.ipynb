{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Machine Translation with ``nn.Transformer`` and torchtext\n",
    "\n",
    "This tutorial is adapted from [the PyTorch Tutorials](https://pytorch.org/tutorials/), valuable references for learning the fundamentals of Natural Language Processing in PyTorch. We highly recommend exploring other tutorials for deeper understanding and additional topics.\n",
    "\n",
    "In this tutorial, you will learn the following:\n",
    "- The process of training a translation model from the ground up using the Transformer architecture.\n",
    "- Utilizing the torchtext library to acquire the [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1) dataset for training a model that translates from German to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sourcing and Processing\n",
    "\n",
    "The [torchtext library](https://pytorch.org/text/stable/) provides convenient tools for constructing datasets that can be easily traversed, particularly for the purpose of developing a language translation model. In this tutorial, we demonstrate the utilization of torchtext's built-in datasets, outlining the process of tokenizing a raw text sentence, constructing a vocabulary, and converting tokens into tensors. We will employ the [Multi30k dataset from the torchtext library](https://pytorch.org/text/stable/datasets.html#multi30k), which supplies pairs of source-target raw sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import sys  # Module for interacting with the Python interpreter\n",
    "\n",
    "# Checking if the code is running in Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(f\"Running in Google Colab ...\")\n",
    "\n",
    "    print(f\"Installing dependencies ...\")\n",
    "    # Install dependencies:\n",
    "    !pip install -U torchtext\n",
    "    !pip install -U spacy\n",
    "    !pip install -U portalocker\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_sm\n",
    "else:\n",
    "    # Code is running in a local setup\n",
    "    print(f\"Running in Local Setup ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and functions from torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the following:\n",
    "- The first two lines set the URLs for the training and validation sets of the Multi30k dataset.\n",
    "- `SRC_LANGUAGE` and `TGT_LANGUAGE` are assigned the strings 'de' and 'en', representing the source (German) and target (English) languages.\n",
    "- `token_transform` and `vocab_transform` are initialized as empty dictionaries, used for transformations in the later code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define URLs for the Multi30k dataset (training and validation sets)\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "# Specify source and target languages\n",
    "SRC_LANGUAGE = 'de'  # German\n",
    "TGT_LANGUAGE = 'en'  # English\n",
    "\n",
    "# Placeholders for token and vocabulary transformations\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tokenizers for the source and target languages.\n",
    "\n",
    "- The `get_tokenizer` is a function from torchtext that retrieves a tokenizer based on the specified method ('spacy' in this case). For the source language (German), the 'de_core_news_sm' spaCy model is used as the tokenizer.\n",
    "\n",
    "- For the target language (English), the 'en_core_web_sm' spaCy model is used as the tokenizer. These tokenizers are stored in the dictionary `token_transform` with keys representing the source and target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign source language tokenizer using the 'spacy' tokenizer with the German language model \n",
    "# 'de_core_news_sm'\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "# Assign target language tokenizer using the 'spacy' tokenizer with the English language model \n",
    "# 'en_core_web_sm'\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is designed to work with language pairs, and it yields tokenized sentences for a specific language.\n",
    "\n",
    "- The function `yield_tokens` is a helper function that takes a data iterator (`data_iter`) and a target language (`language`).\n",
    "- It uses the `token_transform` dictionary to access the appropriate tokenizer for the specified language.\n",
    "- The function iterates through the provided data iterator and yields a list of tokens for each data sample in the specified language.\n",
    "- The `language_index` dictionary is used to map the source and target languages to their respective indices in the data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to yield a list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    A helper function to yield a list of tokens from a given data iterator and language.\n",
    "\n",
    "    Args:\n",
    "        data_iter (Iterable): The data iterator containing language pairs.\n",
    "        language (str): The target language for tokenization.\n",
    "\n",
    "    Yields:\n",
    "        List[str]: A list of tokens from each data sample.\n",
    "    \"\"\"\n",
    "    # Define an index mapping for source and target languages\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    # Iterate through the data iterator\n",
    "    for data_sample in data_iter:\n",
    "        # Use the specified tokenizer for the given language to tokenize the data sample\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These special symbols and indices are commonly used in natural language processing tasks, such as in this case for language translation, to handle unknown words, padding, and sentence boundaries.\n",
    "\n",
    "- Four special symbols are defined: `<unk>` (unknown), `<pad>` (padding), `<bos>` (beginning of sequence), and `<eos>` (end of sequence).\n",
    "- Each special symbol is assigned a unique index. These indices will be used when building the vocabulary to represent these special symbols.\n",
    "- The variables UNK_IDX, PAD_IDX, BOS_IDX, and EOS_IDX store the indices for the respective special symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a vocabulary is a crucial step in natural language processing tasks, enabling the model to map words to unique indices.\n",
    "\n",
    "- The code iterates over the source language (SRC_LANGUAGE) and target language (TGT_LANGUAGE).\n",
    "- For each language, it creates a training data iterator (`train_iter`) for the Multi30k dataset with the specified language pair.\n",
    "- The `build_vocab_from_iterator` function is then used to construct a vocabulary for the current language.\n",
    "- The `yield_tokens` function is employed to tokenize and yield tokens from the training iterator.\n",
    "- Parameters such as `min_freq` (minimum frequency for token inclusion), `specials` (special symbols), and `special_first` (placing special symbols at the beginning) are specified during vocabulary construction.\n",
    "- The resulting vocabulary is stored in the `vocab_transform` dictionary, with keys representing the source and target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over source (SRC_LANGUAGE) and target (TGT_LANGUAGE) languages\n",
    "for lang in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Create a training data iterator for the Multi30k dataset with the specified language pair\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    \n",
    "    # Build a vocabulary using torchtext's Vocab object for the current language\n",
    "    vocab_transform[lang] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, lang),  # Tokenize and yield tokens from the training iterator\n",
    "        min_freq=1,  # Set the minimum frequency threshold for including tokens in the vocabulary\n",
    "        specials=special_symbols,  # Specify special symbols to be included in the vocabulary\n",
    "        special_first=True  # Place special symbols at the beginning of the vocabulary\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the default index to the index of the `<unk>` token is a common practice. It ensures that if a word is not present in the vocabulary, the model will treat it as an unknown token during inference.\n",
    "\n",
    "- The code iterates over the source language (SRC_LANGUAGE) and target language (TGT_LANGUAGE).\n",
    "- For each language, it sets the default index of the corresponding vocabulary to the index of the `<unk>` (unknown) token.\n",
    "- The default index is the index assigned to a token when it is not found in the vocabulary. In this case, it is set to the index of the `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set `UNK_IDX` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws `RuntimeError` when the queried token is not found in the Vocabulary.\n",
    "# Iterate over source (SRC_LANGUAGE) and target (TGT_LANGUAGE) languages\n",
    "for lang in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Set the default index of the vocabulary for the current language to the index of the '<unk>' (unknown) token\n",
    "    vocab_transform[lang].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Network using Transformer\n",
    "\n",
    "The Transformer, a Seq2Seq model introduced in the paper [\"Attention is all you need\"](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), is designed for addressing machine translation tasks. In the following, we will construct a Seq2Seq network that leverages the Transformer architecture. This network comprises three main components.\n",
    "\n",
    "The first component is the embedding layer, responsible for converting a tensor of input indices into a corresponding tensor of input embeddings. These embeddings are then enriched with positional encodings, offering positional information about input tokens to the model.\n",
    "\n",
    "The second component is the actual [Transformer model](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html). This core part of the network utilizes self-attention mechanisms to effectively capture relationships between different parts of the input sequence.\n",
    "\n",
    "Finally, the output from the Transformer model undergoes processing through a linear layer. This layer generates unnormalized probabilities for each token in the target language, providing the model's predictions for the translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and functions from PyTorch\n",
    "from torch import Tensor  # Tensor class for creating and manipulating tensors\n",
    "import torch  # The main PyTorch module\n",
    "import torch.nn as nn  # Neural network module for defining layers and models\n",
    "from torch.nn import Transformer  # Transformer module for implementing the Transformer model\n",
    "import math  # The math module for mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal device for computations, prioritizing GPUs and MPS:\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\"  # Prioritize GPU if available\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"  # use MPS if available\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"  # Fallback to CPU\n",
    ")\n",
    "\n",
    "# Print the selected device for clarity:\n",
    "print(f\"Torch Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding is essential in Transformer models to provide information about the positions of tokens in a sequence, allowing the model to consider the order of words.\n",
    "\n",
    "- The `PositionalEncoding` class is defined as a PyTorch module to add positional encoding to token embeddings.\n",
    "- In the constructor (`__init__` method), denormalization term `den` is computed to be used in the positional encoding formula.\n",
    "- Positions (`pos`) from 0 to `maxlen` are generated, and an initial tensor for positional embeddings (`pos_embedding`) is initialized.\n",
    "- Sine and cosine components of the positional encoding are computed and assigned to the even and odd indices of the `pos_embedding` tensor, respectively.\n",
    "- A batch dimension is added to the `pos_embedding` tensor to match the batch size of the input token embeddings.\n",
    "- The dropout layer is initialized, and the positional embeddings are registered as a buffer in the module.\n",
    "- In the `forward` method, dropout is applied to the sum of the input token embeddings and the corresponding positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute denormalization term for positional encoding\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "\n",
    "        # Generate positions from 0 to maxlen\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "\n",
    "        # Initialize positional embeddings tensor\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "\n",
    "        # Compute sine and cosine components of positional encoding\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "\n",
    "        # Add a batch dimension to positional embeddings\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        # Initialize a dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Register the positional embeddings as a buffer\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # Apply dropout to the sum of token embeddings and positional embeddings\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling by the square root of the embedding size is a common practice to stabilize the training of models using embeddings.\n",
    "\n",
    "- The `TokenEmbedding` class is defined as a PyTorch module to convert a tensor of input indices into the corresponding tensor of token embeddings.\n",
    "- In the constructor (`__init__` method), an embedding layer (`nn.Embedding`) is initialized with a vocabulary size of `vocab_size` and an embedding size of `emb_size`.\n",
    "- The embedding size is stored as an attribute (`self.emb_size`) for later use.\n",
    "- In the `forward` method, the input tokens are passed through the embedding layer, and the result is scaled by the square root of the embedding size.\n",
    "- The `.long()` method is used to ensure that input tokens are treated as long integers, as required by the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper Module to convert a tensor of input indices into the corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "\n",
    "        # Initialize an embedding layer with vocab_size vocabulary and emb_size embedding size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # Store the embedding size for later use\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        # Apply the embedding layer to input tokens, and scale the result by the square root of the embedding size\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code represents a sequence-to-sequence model for machine translation tasks using the Transformer architecture.\n",
    "\n",
    "- The `Seq2SeqTransformer` class is defined as a PyTorch module, representing a sequence-to-sequence model based on the Transformer architecture.\n",
    "- The constructor (`__init__` method) initializes the Transformer model, linear layer, token embedding layers for source and target languages, and positional encoding layer.\n",
    "- The `forward` method defines the forward pass of the model, applying positional encoding to token embeddings, passing them through the Transformer, and generating output probabilities.\n",
    "- The `encode` and `decode` methods separately encode the source sequence and decode the target sequence using the Transformer encoder and decoder, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a Seq2Seq Network using the Transformer architecture\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        # Instantiate the Transformer model with specified parameters\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # Linear layer for generating output probabilities\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "        # Token embedding layer for the source language\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "\n",
    "        # Token embedding layer for the target language\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "\n",
    "        # Positional encoding layer for introducing word order information\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        # Apply positional encoding to token embeddings for source and target sequences\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        # Pass the source and target sequences through the Transformer model\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "        # Generate output probabilities using the linear layer\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        # Encode the source sequence using the Transformer encoder\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        # Decode the target sequence using the Transformer decoder\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, it is crucial to employ a subsequent word mask to prevent the model from peering into future words when generating predictions. Additionally, masks are necessary to conceal padding tokens in both the source and target sequences. Here, we will define a function to handle the creation of these masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used during training to mask future words when making predictions in sequence-to-sequence models, preventing the model from peeking into future tokens.\n",
    "\n",
    "- The function `generate_square_subsequent_mask` creates a square subsequent mask for a given size `sz`.\n",
    "- `torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1` creates an upper triangular matrix of ones using the `torch.triu` function.\n",
    "- `.transpose(0, 1)` transposes the matrix to make it a subsequent mask.\n",
    "- `mask.float()` converts the boolean mask to a float tensor.\n",
    "- `mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))` replaces 0s with negative infinity and 1s with 0.0 using `masked_fill`.\n",
    "- The resulting mask is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    # Create an upper triangular matrix of ones using torch.triu\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "\n",
    "    # Convert the boolean mask to a float tensor\n",
    "    mask = mask.float()\n",
    "\n",
    "    # Use masked_fill to replace 0s with negative infinity and 1s with 0.0\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "\n",
    "    # Return the generated square subsequent mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These masks are used during the training process in sequence-to-sequence models to mask future words and handle padding tokens.\n",
    "\n",
    "- The function `create_mask` takes the source sequence (`src`) and target sequence (`tgt`) as input.\n",
    "- `src_seq_len` and `tgt_seq_len` are calculated as the lengths of the source and target sequences, respectively.\n",
    "- `tgt_mask` is generated using the `generate_square_subsequent_mask` function for the target sequence.\n",
    "- `src_mask` is created as a zero mask for the source sequence, which is often not used in the context of the Transformer model.\n",
    "- `src_padding_mask` and `tgt_padding_mask` are created by checking if the elements are equal to the padding index (`PAD_IDX`) and then transposing the result.\n",
    "- The function returns the source mask, target mask, source padding mask, and target padding mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    # Get the sequence lengths of the source and target sequences\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    # Generate a square subsequent mask for the target sequence\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "\n",
    "    # Create a zero mask for the source sequence\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    # Create padding masks for source and target sequences\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "\n",
    "    # Return the source mask, target mask, source padding mask, and target padding mask\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will specify and instantiate the parameters for our model. Additionally, we will define our loss function, namely the cross-entropy loss, and the optimizer employed during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps set up the model, loss function, and optimizer for training a Seq2Seq Transformer model.\n",
    "\n",
    "- `torch.manual_seed(0)`: Sets a manual seed to ensure reproducibility of the results.\n",
    "- `SRC_VOCAB_SIZE` and `TGT_VOCAB_SIZE`: Obtain the vocabulary sizes for the source and target languages.\n",
    "- Model hyperparameters: Set various hyperparameters for the Seq2Seq Transformer model.\n",
    "- `transformer`: Instantiate the Seq2SeqTransformer model with the specified hyperparameters.\n",
    "- Xavier initialization: Initialize the model parameters using Xavier (Glorot) initialization for better training stability.\n",
    "- Move the model to the specified device (e.g., GPU) using `.to(DEVICE)`.\n",
    "- `loss_fn`: Define the loss function as CrossEntropyLoss, with the padding index ignored during computation.\n",
    "- `optimizer`: Define the optimizer as Adam with a learning rate of 0.0001, beta parameters (0.9, 0.98), and epsilon value 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set a manual seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Get the vocabulary sizes for the source and target languages\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "\n",
    "# Define model hyperparameters\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "# Instantiate the Seq2Seq Transformer model\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "# Initialize the model parameters using Xavier initialization\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# Move the model to the specified device (e.g., GPU)\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "# Define the loss function as CrossEntropyLoss with padding index ignored\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Define the optimizer as Adam with specified learning rate and beta parameters\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collation\n",
    "\n",
    "As demonstrated in the `Data Sourcing and Processing` section, our data iterator produces pairs of raw strings. To facilitate processing by our previously defined `Seq2Seq` network, we must convert these string pairs into batched tensors. Below, we establish our collate function, which transforms a batch of raw strings into tensors suitable for direct input into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pad_sequence function from torch.nn.utils.rnn module\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function is useful for creating a pipeline of text transformations where multiple operations need to be applied in a specific order.\n",
    "\n",
    "- The `sequential_transforms` function is defined as a helper function to combine sequential transformations.\n",
    "- The function takes a variable number of transformations (`*transforms`) as arguments, allowing flexibility in the number of transformations to be applied.\n",
    "- Inside the function, a new function (`func`) is defined. This function takes a text input (`txt_input`) and applies each transformation in the provided list of transforms sequentially.\n",
    "- The result of applying all sequential transformations is returned by the `func` function.\n",
    "- Finally, the `sequential_transforms` function returns the `func` function, which can be used to apply the sequential transformations to text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to combine sequential transformations\n",
    "def sequential_transforms(*transforms):\n",
    "    # The function takes a variable number of transformations as arguments\n",
    "    def func(txt_input):\n",
    "        # For each transformation in the provided list of transforms\n",
    "        for transform in transforms:\n",
    "            # Apply the transformation to the input\n",
    "            txt_input = transform(txt_input)\n",
    "        # Return the result of applying all sequential transformations\n",
    "        return txt_input\n",
    "    # Return the function that applies sequential transformations\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up language-specific text transformations that convert raw strings into tensors with BOS and EOS tokens, which is useful for preparing input sequences for a Seq2Seq model.\n",
    "\n",
    "- `tensor_transform` is a function defined to add the beginning-of-sequence (BOS) and end-of-sequence (EOS) tokens to a list of token indices and create a tensor.\n",
    "- It uses `torch.cat` to concatenate tensors for BOS, the input token indices, and EOS.\n",
    "- An empty dictionary `text_transform` is initialized to store language-specific text transformations.\n",
    "- The code iterates over source (SRC_LANGUAGE) and target (TGT_LANGUAGE) languages.\n",
    "- For each language (`lang`), a language-specific text transformation is defined using the `sequential_transforms` helper function.\n",
    "- The sequential transformations include tokenization (`token_transform[lang]`), numericalization (`vocab_transform[lang-]`), and adding BOS/EOS tokens (`tensor_transform`).\n",
    "- The resulting language-specific text transformations are stored in the `text_transform` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add BOS/EOS and create a tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    # Concatenate tensors for BOS, token_ids, and EOS\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# Initialize an empty dictionary to store language-specific text transformations\n",
    "text_transform = {}\n",
    "\n",
    "# Iterate over source (SRC_LANGUAGE) and target (TGT_LANGUAGE) languages\n",
    "for lang in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Define a language-specific text transformation using sequential transforms\n",
    "    text_transform[lang] = sequential_transforms(token_transform[lang],  # Tokenization\n",
    "                                               vocab_transform[lang],  # Numericalization\n",
    "                                               tensor_transform)     # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collation function is crucial for preparing batches of data during training, ensuring that sequences within a batch have the same length.\n",
    "\n",
    "- `collate_fn` is a function defined to collate data samples into batch tensors, suitable for processing by a Seq2Seq model.\n",
    "- Two empty lists, `src_batch` and `tgt_batch`, are initialized to store source and target batches, respectively.\n",
    "- The function iterates over each (source, target) pair in the input batch.\n",
    "- For each pair, language-specific text transformations (`text_transform`) are applied to both the source and target samples, and the results are appended to the respective batches.\n",
    "- `pad_sequence` is then used to pad sequences in the batches to the length of the longest sequence using the specified padding value (`PAD_IDX`).\n",
    "- The function returns the collated source and target batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    # Initialize empty lists to store source and target batches\n",
    "    src_batch, tgt_batch = [], []\n",
    "\n",
    "    # Iterate over each (source, target) pair in the batch\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        # Apply language-specific text transformations to source and target samples\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    # Use pad_sequence to pad sequences in the batch to the length of the longest sequence\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    # Return the collated source and target batches\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish the training and evaluation loops, which will be invoked for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import statement makes the `DataLoader` class available for use in the current code or script.\n",
    "\n",
    "- `torch.utils.data` is a module in PyTorch that provides utilities for working with data, including datasets and data loaders.\n",
    "- `DataLoader` is a class within the `torch.utils.data` module that is used to load batches of data from a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DataLoader class from the torch.utils.data module\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function represents a single training epoch for a Seq2Seq model and is a fundamental part of the training process.\n",
    "\n",
    "- `train_epoch` is a function designed to train one epoch (iteration over the entire training dataset) of the Seq2Seq model.\n",
    "- The model is set to training mode using `model.train()`.\n",
    "- The function initializes a variable (`losses`) to store the cumulative loss during the epoch.\n",
    "- A DataLoader (`train_dataloader`) is created to iterate over batches in the training dataset.\n",
    "- For each batch, the source and target sequences are moved to the specified device (e.g., GPU).\n",
    "- The last token is removed from the target sequence to create the target input.\n",
    "- Masks for source and target sequences are created using the `create_mask` function.\n",
    "- The model is forwarded with the source and target input sequences, and logits are obtained.\n",
    "- The gradients in the optimizer are zeroed using `optimizer.zero_grad()`.\n",
    "- The first token is removed from the target sequence to create the target output.\n",
    "- The loss is computed using the predicted logits and target output.\n",
    "- Backward pass (`loss.backward()`) and optimization step (`optimizer.step()`) are performed.\n",
    "- The loss is accumulated for monitoring.\n",
    "- The average loss for the epoch is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for training one epoch of the Seq2Seq model\n",
    "def train_epoch(model, optimizer):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize the variable to store the cumulative loss during the epoch\n",
    "    losses = 0\n",
    "\n",
    "    # Create a DataLoader for the training dataset\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    # Iterate over batches in the training DataLoader\n",
    "    for src, tgt in train_dataloader:\n",
    "        # Move the source and target batches to the specified device (e.g., GPU)\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        # Remove the last token from the target sequence to create the target input\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        # Create masks for source and target sequences\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        # Zero out the gradients in the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Remove the first token from the target sequence to create the target output\n",
    "        tgt_out = tgt[1:, :]\n",
    "\n",
    "        # Compute the loss using the predicted logits and target output\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss for monitoring\n",
    "        losses += loss.item()\n",
    "\n",
    "    # Return the average loss for the epoch\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function evaluates the model on the validation set and provides insights into its performance.\n",
    "\n",
    "- `evaluate` is a function designed to evaluate the performance of the Seq2Seq model on the validation set.\n",
    "- The model is set to evaluation mode using `model.eval()`.\n",
    "- The function initializes a variable (`losses`) to store the cumulative loss during evaluation.\n",
    "- A DataLoader (`val_dataloader`) is created to iterate over batches in the validation dataset.\n",
    "- For each batch, the source and target sequences are moved to the specified device (e.g., GPU).\n",
    "- The last token is removed from the target sequence to create the target input.\n",
    "- Masks for source and target sequences are created using the `create_mask` function.\n",
    "- The model is forwarded with the source and target input sequences, and logits are obtained.\n",
    "- The first token is removed from the target sequence to create the target output.\n",
    "- The loss is computed using the predicted logits and target output.\n",
    "- The loss is accumulated for monitoring.\n",
    "- The average loss for the validation set is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function for evaluating the performance of the Seq2Seq model on the validation set\n",
    "def evaluate(model):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the variable to store the cumulative loss during evaluation\n",
    "    losses = 0\n",
    "\n",
    "    # Create a DataLoader for the validation dataset\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    # Iterate over batches in the validation DataLoader\n",
    "    for src, tgt in val_dataloader:\n",
    "        # Move the source and target batches to the specified device (e.g., GPU)\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        # Remove the last token from the target sequence to create the target input\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        # Create masks for source and target sequences\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        # Remove the first token from the target sequence to create the target output\n",
    "        tgt_out = tgt[1:, :]\n",
    "\n",
    "        # Compute the loss using the predicted logits and target output\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        # Accumulate the loss for monitoring\n",
    "        losses += loss.item()\n",
    "\n",
    "    # Return the average loss for the validation set\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, armed with all the necessary components, we are ready to commence the training of our model. Let's proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet represents the training loop over multiple epochs, where the model is trained for each epoch, and the training and validation losses are printed along with the time taken for each epoch.\n",
    "\n",
    "- `from timeit import default_timer as timer`: Import the `default_timer` function from the `timeit` module and alias it as `timer`.\n",
    "- `NUM_EPOCHS = 18`: Set the number of training epochs to 18.\n",
    "- `for epoch in range(1, NUM_EPOCHS+1)`: Iterate over the specified number of epochs.\n",
    "- `start_time = timer()`: Record the start time of the epoch using the `timer` function.\n",
    "- `train_loss = train_epoch(transformer, optimizer)`: Perform one training epoch using the `train_epoch` function and obtain the training loss.\n",
    "- `end_time = timer()`: Record the end time of the epoch using the `timer` function.\n",
    "- `val_loss = evaluate(transformer)`: Evaluate the model on the validation set using the `evaluate` function and obtain the validation loss.\n",
    "- `print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))`: Print the training and validation loss along with epoch information using formatted strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the default_timer function from the timeit module\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Set the number of epochs for training\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "# Iterate over the specified number of epochs\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # Record the start time of the epoch\n",
    "    start_time = timer()\n",
    "\n",
    "    # Perform one training epoch and obtain the training loss\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "\n",
    "    # Record the end time of the epoch\n",
    "    end_time = timer()\n",
    "\n",
    "    # Evaluate the model on the validation set and obtain the validation loss\n",
    "    val_loss = evaluate(transformer)\n",
    "\n",
    "    # Print the training and validation loss along with epoch information\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "           f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used for generating output sequences during inference using the trained Seq2Seq model.\n",
    "\n",
    "- `greedy_decode` is a function for generating an output sequence using the greedy decoding algorithm.\n",
    "- The source sequence (`src`) and source mask (`src_mask`) are moved to the specified device (e.g., GPU).\n",
    "- The source sequence is encoded using the `encode` function of the model.\n",
    "- The target sequence (`ys`) is initialized with the start symbol.\n",
    "- The function iterates until the maximum length is reached or the end-of-sequence token is encountered.\n",
    "- The target sequence is decoded using the `decode` function of the model.\n",
    "- The output tensor is transposed, and the model's generator is used to obtain probabilities for the next word.\n",
    "- The index of the word with the maximum probability is determined.\n",
    "- The next word is appended to the target sequence.\n",
    "- If the end-of-sequence token is encountered, the loop is terminated.\n",
    "- The function returns the generated target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for generating an output sequence using the greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    # Move source and source mask to the specified device (e.g., GPU)\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    # Encode the source sequence using the model\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # Initialize the target sequence with the start symbol\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "\n",
    "    # Iterate until the maximum length is reached or the end-of-sequence token is encountered\n",
    "    for i in range(max_len-1):\n",
    "        # Move memory to the specified device (e.g., GPU)\n",
    "        memory = memory.to(DEVICE)\n",
    "\n",
    "        # Generate a mask for the target sequence\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "\n",
    "        # Decode the target sequence using the model\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "\n",
    "        # Transpose the output tensor\n",
    "        out = out.transpose(0, 1)\n",
    "\n",
    "        # Use the model's generator to obtain probabilities for the next word\n",
    "        prob = model.generator(out[:, -1])\n",
    "\n",
    "        # Find the index of the word with the maximum probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        # Append the next word to the target sequence\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "\n",
    "        # If the end-of-sequence token is encountered, break the loop\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    # Return the generated target sequence\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is useful for translating input sentences using the trained Seq2Seq model.\n",
    "\n",
    "- `translate` is a function designed to translate an input sentence into the target language using a Seq2Seq model.\n",
    "- The model is set to evaluation mode using `model.eval()`.\n",
    "- The source sentence is tokenized and numericalized using the `text_transform` for the source language.\n",
    "- The number of tokens in the source sentence is obtained.\n",
    "- A source mask is created with all values set to `False`.\n",
    "- The `greedy_decode` function is used to generate the target tokens.\n",
    "- The target tokens are converted to a string using the vocabulary transformation for the target language.\n",
    "- The generated string is modified to remove \"<bos>\" and \"<eos>\" tokens.\n",
    "- The translated sentence is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to translate an input sentence into the target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and numericalize the source sentence using the text_transform\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "\n",
    "    # Obtain the number of tokens in the source sentence\n",
    "    num_tokens = src.shape[0]\n",
    "\n",
    "    # Create a source mask with all values set to False\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "\n",
    "    # Use the greedy_decode function to generate the target tokens\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "\n",
    "    # Convert the target tokens to a string using the vocabulary transformation\n",
    "    translated_sentence = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy())))\n",
    "\n",
    "    # Remove \"<bos>\" and \"<eos>\" tokens from the translated sentence\n",
    "    translated_sentence = translated_sentence.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "    # Return the translated sentence\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates an example of translating a German sentence into English using the trained Seq2Seq model.\n",
    "\n",
    "- `translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\")`: Call the `translate` function with the trained Seq2Seq model (`transformer`) and the input sentence in the source language (\"Eine Gruppe von Menschen steht vor einem Iglu .\").\n",
    "- The function processes the input sentence through the model and generates the translation in the target language.\n",
    "- The translated sentence is printed to the console using `print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the translation of the input sentence using the trained Seq2Seq model\n",
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Attention is all you need paper.\n",
    "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
